{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "from tgan.model import TGANModel\n",
    "import argparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['census']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TGANModel' object has no attribute 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26f40d698a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m                      \u001b[0mnum_gen_rnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                      num_gen_feature=64)\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnew_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TGAN_WGAN/tgan/model.py\u001b[0m in \u001b[0;36mprepare_sampling\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;34m\"\"\"Prepare model for generate samples.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TGAN_WGAN/tgan/model.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;34m\"\"\"Return a new instance of the model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         return GraphBuilder(\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mz_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TGANModel' object has no attribute 'metadata'"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "from tgan.model import TGANModel\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def get_data(ds, drop=None, n_unique=20, sep=';', suffix='cat'):\n",
    "    d = pd.read_csv(f'../data/{ds}/{ds}_{suffix}.csv', sep=sep)\n",
    "    if drop is not None:\n",
    "        d = d.drop(drop, axis=1)\n",
    "\n",
    "    continuous_columns = []\n",
    "    for col in d._get_numeric_data().columns:\n",
    "        if len(d[col].unique()) > n_unique:\n",
    "            continuous_columns.append(d.columns.get_loc(col))\n",
    "    return d, continuous_columns\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Evaluate data synthesizers')\n",
    "# parser.add_argument('--dataset', nargs='*', help='Which dataset to choose. Options are berka, creditcard and ticket', default=['berka', 'census', 'creditcard'])\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# datasets = args.dataset\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    if ds == 'berka':\n",
    "        d, continuous_columns = get_data(ds, drop=['trans_bank_partner', 'trans_account_partner'])\n",
    "    elif ds == 'census':\n",
    "        d, continuous_columns = get_data(ds, sep=',')\n",
    "    elif ds == 'creditcard':\n",
    "        d, continuous_columns = get_data(ds, sep=',', suffix='num')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset mentioned')\n",
    "\n",
    "    project_name = \"tgan-wgan-gp\"\n",
    "    experiment = Experiment(api_key=\"49HGMPyIKjokHwg2pVOKWTG67\",\n",
    "                            project_name=project_name, workspace=\"baukebrenninkmeijer\")\n",
    "    experiment.log_parameter('dataset', ds)\n",
    "\n",
    "    tgan = TGANModel(continuous_columns,\n",
    "                     restore_session=False,\n",
    "                     max_epoch=100,\n",
    "                     steps_per_epoch=5000,\n",
    "                     batch_size=200,\n",
    "                     experiment=experiment,\n",
    "                     num_gen_rnn=50,\n",
    "                     num_gen_feature=64)\n",
    "#     tgan.fit(d)\n",
    "\n",
    "    if os.path.exists('/mnt'):\n",
    "        if not os.path.exists('/mnt/model'):\n",
    "            os.mkdir('/mnt/model')\n",
    "        model_path = f'/mnt/model/{ds}_{project_name}'\n",
    "    else:\n",
    "        model_path = f'model/{ds}_{project_name}'\n",
    "\n",
    "    num_samples = 100000\n",
    "    new_samples = tgan.sample(num_samples)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tgan.save(f'/mnt/{model_path}', force=True)\n",
    "    except:\n",
    "        tgan.save(model_path, force=True)\n",
    "\n",
    "    p = new_samples.copy()\n",
    "    if ds == 'berka' or ds == 'census':\n",
    "        p[p._get_numeric_data().columns] = p[p._get_numeric_data().columns].astype('int')\n",
    "    if ds == 'creditcard':\n",
    "        p[['time', 'class']] = p[['time', 'class']].astype('int')\n",
    "\n",
    "    if os.path.exists('/mnt'):\n",
    "        if not os.path.exists('/mnt/samples'):\n",
    "            os.mkdir('/mnt/samples')\n",
    "        p.to_csv(f'/mnt/samples/{ds}_sample_{project_name}.csv', index=False)\n",
    "    else:\n",
    "        p.to_csv(f'samples/{ds}_sample_{project_name}.csv', index=False)\n",
    "        \n",
    "    experiment.end()\n",
    "\n",
    "\n",
    "    import tensorflow as tf\n",
    "    tf.keras.backend.clear_session()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../TGAN_SKIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "from tgan.model import TGANModel\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def get_data(ds, drop=None, n_unique=20, sep=';', suffix='cat'):\n",
    "    d = pd.read_csv(f'../data/{ds}/{ds}_{suffix}.csv', sep=sep)\n",
    "    if drop is not None:\n",
    "        d = d.drop(drop, axis=1)\n",
    "        \n",
    "    continuous_columns = []\n",
    "    for col in d._get_numeric_data().columns:\n",
    "        if len(d[col].unique()) > n_unique:\n",
    "            continuous_columns.append(d.columns.get_loc(col))\n",
    "    return d, continuous_columns\n",
    "\n",
    "\n",
    "datasets = ''\n",
    "# parser = argparse.ArgumentParser(description='Evaluate data synthesizers')\n",
    "# parser.add_argument('--dataset', nargs='*', help='Which dataset to choose. Options are berka, creditcard and ticket', default=['berka', 'census', 'creditcard'])\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# datasets = args.dataset\n",
    "\n",
    "\n",
    "for ds in datasets:\n",
    "    \n",
    "    if ds == 'berka':\n",
    "        d, continuous_columns = get_data(ds, drop=['trans_bank_partner', 'trans_account_partner'])\n",
    "    elif ds == 'census':\n",
    "        d, continuous_columns = get_data(ds, sep=',')    \n",
    "    elif ds == 'creditcard':\n",
    "        d, continuous_columns = get_data(ds, sep=',', suffix='num')\n",
    "    else:\n",
    "        raise Exception('Unknown dataset mentioned')\n",
    "\n",
    "    project_name = \"tgan-skip-connections\"\n",
    "    experiment = Experiment(api_key=\"49HGMPyIKjokHwg2pVOKWTG67\",\n",
    "                            project_name=project_name, workspace=\"baukebrenninkmeijer\")\n",
    "    experiment.log_parameter('dataset', ds)\n",
    "\n",
    "    tgan = TGANModel(continuous_columns, \n",
    "                     restore_session=False,  \n",
    "                     max_epoch=100, \n",
    "                     steps_per_epoch=5000, \n",
    "                     batch_size=200,\n",
    "                     experiment=experiment,\n",
    "                     num_gen_rnn=50,\n",
    "                     num_gen_feature=64)\n",
    "    tgan.fit(d)\n",
    "\n",
    "    if os.path.exists('/mnt'):\n",
    "        if not os.path.exists('/mnt/model'):\n",
    "            os.mkdir('/mnt/model')\n",
    "        model_path = f'/mnt/model/{ds}_{project_name}'\n",
    "    else:\n",
    "        model_path = f'model/{ds}_{project_name}'\n",
    "\n",
    "    num_samples = 100000\n",
    "    new_samples = tgan.sample(num_samples)\n",
    "\n",
    "    p = new_samples.copy()\n",
    "    p.columns = d.columns\n",
    "    if ds == 'berka' or ds == 'census':\n",
    "        p[p._get_numeric_data().columns] = p[p._get_numeric_data().columns].astype('int')\n",
    "    if ds == 'creditcard':\n",
    "        p[['time', 'class']] = p[['time', 'class']].astype('int')\n",
    "\n",
    "    if os.path.exists('/mnt'):\n",
    "        if not os.path.exists('/mnt/samples'):\n",
    "            os.mkdir('/mnt/samples')\n",
    "        p.to_csv(f'/mnt/samples/{ds}_sample_{project_name}.csv', index=False)\n",
    "    else:\n",
    "        p.to_csv(f'samples/{ds}_sample_{project_name}.csv', index=False)\n",
    "        \n",
    "    experiment.end()\n",
    "\n",
    "    try:\n",
    "        tgan.save(f'/mnt/{model_path}', force=True)\n",
    "    except:\n",
    "        tgan.save(model_path, force=True)\n",
    "\n",
    "    import tensorflow as tf\n",
    "    tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "import pandas as pd\n",
    "from tgan.model import TGANModel\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e462aff83756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTGANModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/berka_tgan-skip-connections'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TGAN_WGAN/tgan/model.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/TGANModel'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "model = TGANModel.load('model/berka_tgan-skip-connections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\x1f'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d2a426dda0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/berka_tgan-skip-connections'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\x1f'."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "t = pickle.load(open('model/berka_tgan-skip-connections', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "p3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
